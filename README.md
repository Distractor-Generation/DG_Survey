# DG_Survey
This is reading list for **Distractor Generation for Multiple-Choice Questions: A Survey of Methods, Datasets, and Evaluation**
It contains (Dataset Links, Recommended Research Studies)
## Dataset - [Paper] (Publisher) [Dataset Link]
* CLOTH  - [Large-scale Cloze Test Dataset Created by Teachers] (EMNLP) [Dataset](https://www.cs.cmu.edu/~glai1/data/cloth/)
* SCDE   - [SCDE: Sentence Cloze Dataset with High Quality Distractors From Examinations] (ACL) [Dataset](https://vgtomahawk.github.io/sced.html)
* DGen   - [Knowledge-Driven Distractor Generation for Cloze-Style Multiple Choice Questions] (AAAI) [Dataset](https://github.com/DRSY/DGen)
* CELA - [Cloze Quality Estimation for Language Assessment] (EACL) [Dataset](https://github.com/zz-zhang/cloze-quality-estimation)
* SciQ - [Crowdsourcing Multiple Choice Science Questions] (WNUT) [Dataset](https://allenai.org/data/sciq)
* AQUA-RAT  - [Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems] (ACL) [Dataset](https://github.com/google-deepmind/AQuA)
* OpenBookQA  - [Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering] (EMNLP) [Dataset](https://allenai.org/data/open-book-qa)
* ARC - [Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge] (AI2) [Dataset](https://allenai.org/data/arc)
* CommonsenseQA - [CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge] (NAACL) [Dataset](https://www.tau-nlp.sites.tau.ac.il/commonsenseqa)
* MCQL - [Distractor Generation for Multiple Choice Questions Using Learning to Rank] (BEA) [Dataset](https://github.com/harrylclc/LTR-DG)
* MathQA  - [MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms] (NAACL) [Dataset](https://math-qa.github.io/)
* QASC  - [QASC: A Dataset for Question Answering via Sentence Composition] (AAAI) [Dataset](https://allenai.org/data/qasc)
* MedMCQA  - [MedMCQA: A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering] (PMLR) [Dataset](https://github.com/MedMCQA/MedMCQA?tab=readme-ov-file)
* Televic - [Learning to Reuse Distractors to Support Multiple-Choice Question Generation in Education] (IEEE Transactions on Learning Technologies) [Dataset](https://ieee-dataport.org/documents/distractor-retrieval-dataset), [Sample Test](https://github.com/semerekiros/dist-retrieval)
* EduQG   - [EduQG: A Multi-Format Multiple-Choice Dataset for the Educational Domain] (IEEE Access) [Dataset](https://github.com/hadifar/question-generation)
* ChildrenBookTest  - [The Goldilocks principle: Reading childrenâ€™s books with explicit memory representations] (ICLR) [Dataset](https://github.com/facebookresearch/ParlAI/tree/main/parlai/tasks/cbt)
* WhoDidWhat  - [Whodid What: ALarge-Scale Person-Centered Cloze Dataset] (EMNLP) [Dataset]()
* MCTest - [MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text ] (EMNLP) [Dataset]()
* RACE - [RACE: Large-scale ReAding Comprehension Dataset From Examinations] (EMNLP) [Dataset]()
* RACE-C - [A New Multi-choice Reading Comprehension Dataset for Curriculum Learning] (PMLR) [Dataset]()
* DREAM - [DREAM: A Challenge Data Set and Models for Dialogue-Based Reading Comprehension] (TACL) [Dataset]()
* CosmosQA - [Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning] (EMNLP) [Dataset]()
* ReClor - [ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning] (ICLR) [Dataset]()
* QuAIL - [Getting Closer to AI Complete Question Answering: A Set of Prerequisite Real Tasks] (AAAI) [Dataset]()
* MovieQA - [MovieQA: Understanding Stories in Movies Through Question-Answering] (CVPR) [Dataset]()
* Visual7W - [Visual7W: Grounded Question Answering in Images] (CVPR) [Dataset]()
* TQA - [Are You Smarter Than a Sixth Grader? Textbook Question Answering for Multimodal Machine Comprehension] (CVPR) [Dataset]()
* RecipeQA - [RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cooking Recipes] (EMNLP) [Dataset]()
* ScienceQA - [Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering] (NeurIPS) [Dataset]()





## Recommended Research Studies
